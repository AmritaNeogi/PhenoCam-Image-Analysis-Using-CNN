# -*- coding: utf-8 -*-
"""Phenophase_ResNet50.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gsMcWPiCdCchoQU8Qbk02Rbxbw6yatyt
"""

from google.colab import drive
drive.mount('/content/drive')

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import models, transforms
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.model_selection import train_test_split
from PIL import Image
import pandas as pd
from sklearn.preprocessing import StandardScaler

"""## Load Train, Test, Validation Set"""

# Load the labeled data from the CSV file
labeled_data_path = '/content/drive/MyDrive/Capstone/data_out/labeled_data.csv'
labeled_data = pd.read_csv(labeled_data_path)

# Assuming you have a column 'Filename' that contains the file paths
#image_paths = labeled_data['Filename'].tolist()

# Assuming you have a column 'Label' that contains the labels
#labels = labeled_data['Label'].tolist()

# Split the data into train, val, and test sets

train_paths, val_test_paths, train_labels, val_test_labels = train_test_split(labeled_data['Filename'], labeled_data['DaysSinceSOS'], test_size=0.3, random_state=42)

val_paths, test_paths, val_labels, test_labels = train_test_split(val_test_paths, val_test_labels, test_size=0.5, random_state=42)

# Display the number of samples in each set
print(f"Train set size: {len(train_paths)}")
print(f"Validation set size: {len(val_paths)}")
print(f"Test set size: {len(test_paths)}")

class LeafPhenologyDataset(Dataset):
    def __init__(self, image_paths, labels, transform=None):
        self.image_paths = image_paths
        self.labels = labels
        self.transform = transform
        self.label_scaler = StandardScaler()

        # Check if labels are provided before transforming
        if labels is not None:
            self.labels = self.label_scaler.fit_transform(labels.values.reshape(-1, 1)).flatten()
        else:
            self.labels = None

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img = Image.open(self.image_paths.iloc[idx]).convert('RGB')

        # Check if labels are None, return only the image
        if self.labels is not None:
            label = torch.tensor(self.labels[idx], dtype=torch.float32)
        else:
            label = None

        if self.transform:
            img = self.transform(img)

        return img, label

# # Define the classification model
# class LeafPhenologyModel(nn.Module):
#     def __init__(self, backbone_model):
#         super(LeafPhenologyModel, self).__init__()
#         self.features = nn.Sequential(*list(backbone_model.children())[:-1])
#         self.classification_head = nn.Linear(in_features=backbone_model.fc.in_features, out_features=2)

#     def forward(self, x):
#         x = self.features(x)
#         x = x.view(x.size(0), -1)
#         x = self.classification_head(x)
#         return x

# Define the regression model
class LeafPhenologyModel(nn.Module):
    def __init__(self, backbone_model):
        super(LeafPhenologyModel, self).__init__()
        self.features = nn.Sequential(*list(backbone_model.children())[:-1])
        self.regression_head = nn.Linear(in_features= backbone_model.fc.in_features, out_features=1)

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.regression_head(x)
        return x

# Function to evaluate the model
def evaluate_model(model, dataloader, device):
    model.eval()
    y_true = []
    y_pred = []

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            y_true.extend(labels.cpu().numpy())
            y_pred.extend(outputs.cpu().numpy())

    r2 = r2_score(y_true, y_pred)
    mae = mean_absolute_error(y_true, y_pred)
    rmse = mean_squared_error(y_true, y_pred, squared=False)

    return r2, mae, rmse

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Set image size and transformation
image_size = 224
transform = transforms.Compose([
    transforms.Resize((image_size, image_size)),
    transforms.ToTensor(),
])

# Create datasets and dataloaders
train_dataset = LeafPhenologyDataset(train_paths, train_labels, transform)
val_dataset = LeafPhenologyDataset(val_paths, val_labels, transform)
test_dataset = LeafPhenologyDataset(test_paths, test_labels, transform)

# Adjust batch size based on your requirements
batch_size = 128
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Initialize and load ResNet50 model
resnet50_model = models.resnet50(pretrained=True)
resnet50_model = LeafPhenologyModel(resnet50_model).to(device)

# Define loss function and optimizer
criterion = nn.MSELoss()
optimizer_resnet50 = torch.optim.Adam(resnet50_model.parameters(), lr=0.001)

print(labeled_data.isnull().sum())

print(labeled_data.shape)

num_epochs = 10

for epoch in range(num_epochs):
    # Training loop for ResNet50
    resnet50_model.train()
    for inputs, labels in train_dataloader:
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer_resnet50.zero_grad()
        outputs_resnet50 = resnet50_model(inputs)
        loss_resnet50 = criterion(outputs_resnet50.squeeze(), labels)
        loss_resnet50.backward()
        optimizer_resnet50.step()

    # Check shapes
    # print(outputs_resnet50.shape, labels.shape)


    # Evaluate models on validation set
    r2_resnet50, mae_resnet50, rmse_resnet50 = evaluate_model(resnet50_model, val_dataloader, device)

    print(f'Epoch {epoch + 1}/{num_epochs} - ResNet50: R2={r2_resnet50:.4f}, MAE={mae_resnet50:.4f}, RMSE={rmse_resnet50:.4f}')

"""##Model Evaluation"""

# Evaluate the model on the test set
r2_test, mae_test, rmse_test = evaluate_model(resnet50_model, test_dataloader, device)
print(f'Test Set Evaluation - R2: {r2_test:.4f}, MAE: {mae_test:.4f}, RMSE: {rmse_test:.4f}')

"""## Visualize"""

import matplotlib.pyplot as plt
import numpy as np

def visualize_predictions(model, dataloader, device):
    model.eval()
    y_true = []
    y_pred = []

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            y_true.extend(labels.cpu().numpy())
            y_pred.extend(outputs.cpu().numpy())

    # Convert lists to NumPy arrays for easier manipulation
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)

    # Scatter plot with true values in green and predicted values in red
    plt.scatter(y_true, y_pred, alpha=0.5, c='blue', label='Predicted')
    plt.plot(y_true, y_true, color='green', linestyle='--', linewidth=2, label='True')

    # Add legend
    plt.legend()

    plt.title('Model Predictions vs True Labels')
    plt.xlabel('True Labels')
    plt.ylabel('Predicted Labels')
    plt.show()

# Visualize predictions on the test set
visualize_predictions(resnet50_model, test_dataloader, device)

"""## Observation

**Training Set:**
- **Epochs 1-3:** The model performance on the training set is quite poor, with negative R2 values and high MAE and RMSE. This suggests the model struggles to fit the training data during these initial epochs.
  
- **Epochs 4-7:** Substantial improvement is observed. R2 becomes positive, indicating the model is explaining variance better. MAE and RMSE decrease, suggesting better alignment between predicted and actual values.

- **Epochs 8-10:** The model's performance stabilizes, with fluctuations in R2 and slight increases in MAE and RMSE. This could indicate a potential for overfitting, especially as R2 decreases in Epoch 10.

**Test Set:**
- **R2: 0.2955:** The R2 value on the test set is positive but considerably lower than the highest R2 achieved during training (Epoch 7). This could imply that the model doesn't generalize well to unseen data.

- **MAE and RMSE:** Both MAE and RMSE on the test set are higher than the corresponding values on the training set, indicating that the model's predictive accuracy is lower on new data.

**Inference:**
- The model shows signs of overfitting, especially in the later epochs. The decreasing trend of R2 in the last few epochs of training and the performance gap between the training and test sets suggest that the model might be fitting noise in the training data instead of learning the underlying patterns.

- To address overfitting, regularization techniques (such as dropout or weight regularization) or early stopping might be considered.

- Further model tuning and evaluation on a validation set could help identify the optimal epoch for model deployment.

In summary, while the model performs well on the training set initially, it exhibits signs of overfitting, resulting in suboptimal generalization to the test set. Fine-tuning and additional regularization methods are recommended to improve overall performance and generalization.
"""