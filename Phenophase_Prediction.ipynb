{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "59ssjmmucjqg"
      ],
      "mount_file_id": "1ohpSwfGs5kyxMvI5EHH4nHGpP6XN6vKg",
      "authorship_tag": "ABX9TyPoE/MGOoQJTimIuMK9u1kY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmritaNeogi/PhenoCam-Image-Analysis-Using-CNN/blob/main/Phenophase_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7emP8ghNWCnr",
        "outputId": "3f87a80d-3c26-435f-eebc-a1947efe19aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NYnFN0ssFOE",
        "outputId": "3034fe0f-e56c-42ff-bcf0-0186b124f956"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.1.0+cu118)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchvision) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchvision) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y979EO_EH8dm",
        "outputId": "e9cb3948-e288-46b1-9bb2-f050f18add9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.2)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAyA5Y50IHGJ",
        "outputId": "932c50e2-17da-4773-f829-b15db4460438"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "uDf8Ed1pshrE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Non-Hazy Images\n",
        "We already possess the code for handling 32 misty files. We will analyze our dataset to identify all the files that are free from haze or fog, and subsequently, we will store the images in a CSV file."
      ],
      "metadata": {
        "id": "1evIiN3ZsmyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "not_foggy_file = \"/content/drive/MyDrive/Capstone/data_out/not_foggy/NEON.D01.HARV.DP1.00033/2017.csv\"\n",
        "\n",
        "with open(not_foggy_file, mode='r') as csv_file:\n",
        "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
        "    line_count = 0\n",
        "    for row in csv_reader:\n",
        "        if line_count == 0:\n",
        "            line_count += 1\n",
        "        else:\n",
        "            line_count += 1\n",
        "        img_name = row[\"file\"]\n",
        "        print(img_name)\n",
        "        begin_cv_img = cv2.imread(f\"/content/drive/MyDrive/Capstone/data_raw/phenocamdata/{img_name}\")\n",
        "        begin_rgb = cv2.cvtColor(begin_cv_img, cv2.COLOR_BGR2RGB)\n",
        "        plt.imshow(begin_rgb)\n",
        "        plt.show()\n",
        "\n",
        "    print(f'Processed {line_count} lines.')"
      ],
      "metadata": {
        "id": "bXKf5CxNsiYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We already possess the code for handling 32 misty files. We will analyze our dataset to identify all the files that are free from haze or fog, and here we are saving the images in a folder."
      ],
      "metadata": {
        "id": "mESxP45w0jBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the output folder if it doesn't exist\n",
        "# Define the output folder\n",
        "output_folder = \"/content/drive/MyDrive/Capstone/data_working/images_2017\"\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "not_foggy_file = \"/content/drive/MyDrive/Capstone/data_out/not_foggy/NEON.D01.HARV.DP1.00033/2017.csv\"\n",
        "with open(not_foggy_file, mode='r') as csv_file:\n",
        "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
        "    line_count = 0\n",
        "    for row in csv_reader:\n",
        "        if line_count == 0:\n",
        "            line_count += 1\n",
        "        else:\n",
        "            line_count += 1\n",
        "        img_name = row[\"file\"]\n",
        "        print(img_name)\n",
        "        begin_cv_img = cv2.imread(f\"/content/drive/MyDrive/Capstone/data_raw/phenocamdata/{img_name}\")\n",
        "        begin_rgb = cv2.cvtColor(begin_cv_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Save the image to the output folder in JPG format with the original filename\n",
        "        output_path = os.path.join(output_folder, os.path.basename(img_name))\n",
        "        print(f'Saving image to: {output_path}')\n",
        "        cv2.imwrite(output_path, cv2.cvtColor(begin_rgb, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "    print(f'Processed {line_count} lines.')"
      ],
      "metadata": {
        "id": "kpyqq5yk0fwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Labels using K-mean Clustering (Not working)"
      ],
      "metadata": {
        "id": "cPeY6oMl0pbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_images(directory):\n",
        "    \"\"\"\n",
        "    This function reads and shows images from a specified directory.\n",
        "    It returns a list of images.\n",
        "    \"\"\"\n",
        "    images = []  # List to store the images\n",
        "\n",
        "    # Iterate through each file in the directory\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
        "            # Construct the full path to the image file\n",
        "            filepath = os.path.join(directory, filename)\n",
        "\n",
        "            # Read and normalize the image\n",
        "            image = plt.imread(filepath) / 255.0\n",
        "\n",
        "            print(f\"Image shape: {image.shape}\")\n",
        "\n",
        "            # Show the image\n",
        "            plt.imshow(image)\n",
        "            plt.show()\n",
        "\n",
        "            # Append the image to the list\n",
        "            images.append(image)\n",
        "\n",
        "    return images\n",
        "\n",
        "# Specify the directory containing the images\n",
        "image_directory = \"/content/drive/MyDrive/Capstone/data_working/images_2017\"\n",
        "\n",
        "\n",
        "# Call the function to read and display images\n",
        "pic = read_images(image_directory)\n"
      ],
      "metadata": {
        "id": "m_9UZQqA0lnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_shape(directory, batch_size=10):\n",
        "    \"\"\"\n",
        "    This function reads the images from the specific directory, appends them to an image list,\n",
        "    and returns the shape of the first image in the list.\n",
        "    \"\"\"\n",
        "    images = []  # List to store the images\n",
        "\n",
        "    # Iterate through each file in the directory\n",
        "    for filename in os.listdir(directory):\n",
        "        # Check for acceptable file extensions\n",
        "        if filename.endswith((\".jpg\", \".png\")):\n",
        "            # Construct the full path to the image file\n",
        "            filepath = os.path.join(directory, filename)\n",
        "\n",
        "            try:\n",
        "                # Read and normalize the image\n",
        "                image = plt.imread(filepath) / 255.0\n",
        "                # Append the image to the list\n",
        "                images.append(image)\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading {filename}: {e}\")\n",
        "\n",
        "            # Load images in batches\n",
        "            if len(images) == batch_size:\n",
        "                break\n",
        "\n",
        "    if images:\n",
        "        return images[0].shape\n",
        "    else:\n",
        "        return None  # Return None if no valid images are found\n",
        "\n",
        "# directory containing images\n",
        "image_directory = \"/content/drive/MyDrive/Capstone/data_working/images_2017\"\n",
        "\n",
        "img = image_shape(image_directory, batch_size=10)\n",
        "print(f\"Shape of the first image: {img}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uD7nykjF1VMz",
        "outputId": "47d58a4f-a6b4-42af-d7ec-98306d426449"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the first image: (960, 1296, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load and preprocess images\n",
        "def load_and_preprocess_images(image_directory):\n",
        "    images = []\n",
        "\n",
        "    # # Iterate over subdirectories (assuming they represent months)\n",
        "    # for month_folder in os.listdir(image_directory):\n",
        "    #     month_path = os.path.join(image_directory, month_folder)\n",
        "\n",
        "    #     # Check if it's a directory\n",
        "    #     if os.path.isdir(month_path):\n",
        "    #         print(f\"Loading images from {month_folder}...\")\n",
        "\n",
        "    for filename in os.listdir(image_directory):\n",
        "      if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
        "        img_path = os.path.join(image_directory, filename)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        new_height = 224\n",
        "        aspect_ratio = img.shape[1] / img.shape[0]\n",
        "        new_width = int(new_height * aspect_ratio)\n",
        "        img_resized = cv2.resize(img, (new_width, new_height))\n",
        "        img_normalized = img_resized / 255.0\n",
        "\n",
        "        images.append(img_normalized)\n",
        "\n",
        "        #print(f\"Number of images loaded from {month_folder}: {len(images)}\")\n",
        "\n",
        "    return np.array(images)\n"
      ],
      "metadata": {
        "id": "QCs5bOjv2SqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import MiniBatchKMeans\n"
      ],
      "metadata": {
        "id": "FiOnvwuWld6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the main directory containing subfolders for each month\n",
        "#image_directory = \"/content/drive/MyDrive/Capstone/data_working/images_2017\"\n",
        "#January 2017\n",
        "image_directory = \"/content/drive/MyDrive/Capstone/data_raw/phenocamdata/NEON.D01.HARV.DP1.00033/2017/01\"\n",
        "\n",
        "# Load and preprocess images\n",
        "images = load_and_preprocess_images(image_directory)\n",
        "\n",
        "# Print debug information\n",
        "print(f\"Total number of images loaded: {len(images)}\")\n",
        "\n",
        "# Flatten the images\n",
        "images_flattened = images.reshape(images.shape[0], -1)\n",
        "\n",
        "# Standardize the pixel values\n",
        "scaler = StandardScaler()\n",
        "images_flattened_standardized = scaler.fit_transform(images_flattened)\n",
        "print(\"Shape of images_flattened_standardized:\", images_flattened_standardized.shape)\n",
        "\n",
        "# Use Mini-Batch K-means clustering to determine labels\n",
        "num_clusters = 5  # Adjust the number of clusters based on your phenological stages\n",
        "batch_size = 100  # Adjust the batch size\n",
        "minibatch_kmeans = MiniBatchKMeans(n_clusters=num_clusters, batch_size=batch_size, random_state=42, n_init='auto')\n",
        "labels = minibatch_kmeans.fit_predict(images_flattened_standardized)\n",
        "print(\"Labels:\", labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NyjVTORv3my",
        "outputId": "dca54b63-e438-4e5d-ed8a-203894bf01a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of images loaded: 194\n",
            "Shape of images_flattened_standardized: (194, 202944)\n",
            "Labels: [3 3 3 2 2 3 2 1 2 0 2 1 1 3 2 0 0 0 2 0 0 0 2 0 0 0 2 0 2 2 3 0 3 3 2 3 2\n",
            " 0 0 0 2 2 3 1 1 3 2 3 3 3 3 1 1 0 0 2 0 0 0 3 0 0 0 3 0 0 2 2 0 2 2 2 2 1\n",
            " 2 1 3 2 2 2 3 3 2 3 2 3 1 3 2 3 3 2 4 2 2 2 2 2 2 2 0 2 0 1 3 0 0 1 3 1 1\n",
            " 3 0 3 2 4 3 3 2 3 3 2 3 3 3 3 1 3 1 1 3 4 1 1 1 3 1 1 3 4 3 2 3 2 2 3 2 3\n",
            " 2 2 0 0 0 0 0 3 0 0 0 0 3 2 2 2 2 3 2 2 2 2 3 2 2 2 2 2 3 2 2 2 1 3 2 2 2\n",
            " 1 3 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the main directory containing images\n",
        "#image_directory = \"/content/drive/MyDrive/Capstone/data_raw/phenocamdata/NEON.D01.HARV.DP1.00033/2017/01\"\n",
        "image_directory = \"/content/drive/MyDrive/Capstone/data_working/images_2017\"\n",
        "# Load and preprocess images\n",
        "images = load_and_preprocess_images(image_directory)\n",
        "\n",
        "# Print debug information\n",
        "print(f\"Total number of images loaded: {len(images)}\")\n",
        "\n",
        "# Flatten the images\n",
        "images_flattened = images.reshape(images.shape[0], -1)\n",
        "\n",
        "# Standardize the pixel values\n",
        "scaler = StandardScaler()\n",
        "images_flattened_standardized = scaler.fit_transform(images_flattened)\n",
        "print(\"Shape of images_flattened_standardized:\", images_flattened_standardized.shape)\n",
        "\n",
        "# Use Mini-Batch K-means clustering to determine labels\n",
        "num_clusters = 5  # Adjust the number of clusters based on your phenological stages\n",
        "batch_size = 100  # Adjust the batch size\n",
        "minibatch_kmeans = MiniBatchKMeans(n_clusters=num_clusters, batch_size=batch_size, random_state=42, n_init='auto')\n",
        "cluster_labels = minibatch_kmeans.fit_predict(images_flattened_standardized)\n",
        "\n",
        "# Use the cluster centroids as continuous labels\n",
        "cluster_centroids = minibatch_kmeans.cluster_centers_"
      ],
      "metadata": {
        "id": "-C3OGpbNzhU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clustering to Label dataset is not working"
      ],
      "metadata": {
        "id": "xJt7e5XkR_QB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Doing Simple Sequential Labeling of the non-foggy Dataset"
      ],
      "metadata": {
        "id": "NEM3Bw1wTguC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Load your CSV file with image filenames\n",
        "csv_file = \"/content/drive/MyDrive/Capstone/data_out/not_foggy/NEON.D01.HARV.DP1.00033/2017.csv\"\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# Create a new column for labels\n",
        "df[\"Label\"] = None\n",
        "\n",
        "# Assign labels based on the order of filenames\n",
        "label_counter = 1\n",
        "for index, row in df.iterrows():\n",
        "    # Assuming 'file' column contains the full path to the images\n",
        "    filename = row['file']\n",
        "\n",
        "    # Extract only the filename from the path\n",
        "    image_name = os.path.basename(filename)\n",
        "\n",
        "    # Assign the label\n",
        "    df.at[index, 'Label'] = label_counter\n",
        "    label_counter += 1\n",
        "\n",
        "# Keep only the 'file' and 'Label' columns\n",
        "df = df[['file', 'Label']]\n",
        "\n",
        "# Specify the directory to save the Labeled CSV file\n",
        "output_directory = \"/content/drive/MyDrive/Capstone/data_out/not_foggy/NEON.D01.HARV.DP1.00033/\"\n",
        "\n",
        "# Save the Labeled DataFrame to a new CSV file in the specified directory\n",
        "Labeled_csv_file = os.path.join(output_directory, \"Labeled_data.csv\")\n",
        "df.to_csv(Labeled_csv_file, index=False)"
      ],
      "metadata": {
        "id": "cwuJkrcjPLSy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train, Test, Validation set"
      ],
      "metadata": {
        "id": "ikHpXkU5rBRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from shutil import copyfile\n",
        "\n",
        "# Define paths\n",
        "image_directory = \"/content/drive/MyDrive/Capstone/data_working/images_2017\"\n",
        "label_csv_path = \"/content/drive/MyDrive/Capstone/data_out/not_foggy/NEON.D01.HARV.DP1.00033/Labeled_data.csv\"\n",
        "\n",
        "# Load labels from CSV\n",
        "labels_df = pd.read_csv(label_csv_path)\n",
        "\n",
        "# Display the column names in your DataFrame\n",
        "print(\"Column names in the DataFrame:\", labels_df.columns)\n",
        "\n",
        "# Split the dataset into training, testing, and validation sets\n",
        "train_df, test_df = train_test_split(labels_df, test_size=0.2, random_state=42)\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
        "\n",
        "print(\"Done Splitting\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PswKuUsqrJ_u",
        "outputId": "2d428f7e-f74b-4603-9c21-5002c4e98dc7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column names in the DataFrame: Index(['file', 'Label'], dtype='object')\n",
            "Done Splitting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define output directories for the sets\n",
        "output_directory = \"/content/drive/MyDrive/Capstone/data_out/not_foggy/NEON.D01.HARV.DP1.00033\"\n",
        "train_directory = os.path.join(output_directory, \"train\")\n",
        "test_directory = os.path.join(output_directory, \"test\")\n",
        "val_directory = os.path.join(output_directory, \"validation\")\n",
        "\n",
        "# Create output directories if they don't exist\n",
        "os.makedirs(train_directory, exist_ok=True)\n",
        "os.makedirs(test_directory, exist_ok=True)\n",
        "os.makedirs(val_directory, exist_ok=True)"
      ],
      "metadata": {
        "id": "5L8CrUqmrYR6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy images to respective directories based on the sets\n",
        "def copy_images(df, source_dir, dest_dir):\n",
        "    for index, row in df.iterrows():\n",
        "        image_filename = row['file'].split(\"/\")[-1]  # Extract the last part of the path\n",
        "        source_path = os.path.join(source_dir, image_filename)\n",
        "        dest_path = os.path.join(dest_dir, image_filename)\n",
        "        copyfile(source_path, dest_path)\n",
        "\n",
        "# Copy training set images\n",
        "copy_images(train_df, image_directory, train_directory)\n",
        "\n",
        "# Copy testing set images\n",
        "copy_images(test_df, image_directory, test_directory)\n",
        "\n",
        "# Copy validation set images\n",
        "copy_images(val_df, image_directory, val_directory)\n",
        "\n",
        "# Display the number of samples in each set\n",
        "print(f\"Training set size: {len(train_df)}\")\n",
        "print(f\"Testing set size: {len(test_df)}\")\n",
        "print(f\"Validation set size: {len(val_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNcJrlsdsP-b",
        "outputId": "781aec17-95e0-4173-960b-23667a675163"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 2005\n",
            "Testing set size: 558\n",
            "Validation set size: 223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Augmentation (if needed)"
      ],
      "metadata": {
        "id": "59ssjmmucjqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.preprocessing import image\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set the path to your dataset\n",
        "#data_path = \"/content/drive/MyDrive/Capstone/data_working/images_2017\"\n",
        "#data_path = \"/content/drive/MyDrive/Capstone/data_raw/HARV\"\n",
        "data_path = \"/content/drive/MyDrive/Capstone/data_out/not_foggy/NEON.D01.HARV.DP1.00033/train\"\n",
        "\n",
        "# Create an ImageDataGenerator with augmentation configurations\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Output directory for augmented images\n",
        "output_dir = '/content/drive/MyDrive/Capstone/data_out/augmented_HARV_train'\n",
        "\n",
        "\n",
        "# Iterate through each image in the dataset and apply augmentation\n",
        "for filename in os.listdir(data_path):\n",
        "    img_path = os.path.join(data_path, filename)\n",
        "    img = image.load_img(img_path, target_size=(224, 224))  # Assuming ResNet input size\n",
        "    x = image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "\n",
        "     # Generate augmented images\n",
        "    i = 0\n",
        "    for batch in datagen.flow(x, batch_size=1, save_to_dir=output_dir, save_prefix=filename.split('.')[0], save_format='jpeg'):\n",
        "        # Display at least one augmented image\n",
        "        if i == 0:\n",
        "            augmented_img = image.array_to_img(batch[0])\n",
        "            plt.imshow(augmented_img)\n",
        "            plt.show()\n",
        "\n",
        "        i += 1\n",
        "        if i > 5:  # Adjust the number of augmented images per original image\n",
        "            break"
      ],
      "metadata": {
        "id": "Qo6JuO-wSN9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Building"
      ],
      "metadata": {
        "id": "bHzMVlGk5BCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data.dataset import random_split\n",
        "import os"
      ],
      "metadata": {
        "id": "b4mj_LpE4_s3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the dataset path\n",
        "train_path = \"/content/drive/MyDrive/Capstone/data_out/not_foggy/NEON.D01.HARV.DP1.00033/train\"\n",
        "test_path = \"/content/drive/MyDrive/Capstone/data_out/not_foggy/NEON.D01.HARV.DP1.00033/test\"\n",
        "valid_path = \"/content/drive/MyDrive/Capstone/data_out/not_foggy/NEON.D01.HARV.DP1.00033/validation\"\n",
        "label_csv_path = \"/content/drive/MyDrive/Capstone/data_out/not_foggy/NEON.D01.HARV.DP1.00033/Labeled_data.csv\"\n",
        "# Define the transformation for the dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "id": "JNF9T7HnQGld"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_path, label_csv_path, transform=None):\n",
        "        self.data_path = data_path\n",
        "        self.label_csv_path = label_csv_path\n",
        "        self.transform = transform\n",
        "\n",
        "        # Read the labeled CSV file\n",
        "        self.df = pd.read_csv(label_csv_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        for index, row in self.df.iterrows():  # Fix this line\n",
        "            img_filename = row['file'].split(\"/\")[-1]\n",
        "            img_path = os.path.join(self.data_path, img_filename)\n",
        "            img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "\n",
        "            # Extract the label from the CSV file\n",
        "            label = self.df.at[idx, 'Label']\n",
        "\n",
        "            return img, label\n"
      ],
      "metadata": {
        "id": "fUV--PdSQRi1"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create instances of the CustomDataset with label CSV file\n",
        "train_dataset = CustomDataset(train_path, label_csv_path, transform=transform)\n",
        "test_dataset = CustomDataset(test_path, label_csv_path, transform=transform)\n",
        "valid_dataset = CustomDataset(valid_path, label_csv_path, transform=transform)"
      ],
      "metadata": {
        "id": "TtSc9X6JREAN"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Verify the first batch to see its structure\n",
        "for images, labels in train_loader:\n",
        "    print(\"Batch Structure:\")\n",
        "    print(f\"Images: {images.shape}, Labels: {labels}\")\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZtwtRs6RI_s",
        "outputId": "45f7d543-ab94-4aa8-bc90-bc79d31deccd"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Structure:\n",
            "Images: torch.Size([32, 3, 224, 224]), Labels: tensor([ 794, 1074,  254, 2638, 1175, 1340, 2662, 1641, 2484, 2599, 1302, 2629,\n",
            "        2469,  677,  935,  736,  467, 1151, 1917,  494,  152, 2589, 2057,  398,\n",
            "        1706, 2626, 1371, 1344, 1520, 2092, 2418,  978])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model building functions"
      ],
      "metadata": {
        "id": "gat-9P1GTKxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "# Define the convolutional functions\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "# Define the Bottleneck block\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = conv1x1(inplanes, planes)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = conv3x3(planes, planes, stride)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = conv1x1(planes, planes * self.expansion)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Define the Basic block\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Define the ResNet architecture\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.inplanes = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "opwmEDnqUCgR"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_urls = {\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "   }\n",
        "\n",
        "\n",
        "# ResNet-50\n",
        "def resnet50(pretrained=False, **kwargs):\n",
        "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n",
        "    return model\n",
        "\n",
        "# ResNet-101\n",
        "def resnet101(pretrained=False, **kwargs):\n",
        "    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))\n",
        "    return model"
      ],
      "metadata": {
        "id": "B5hPg6jYS--m"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train Model"
      ],
      "metadata": {
        "id": "ex7YkIbBV2mN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def train_model(model, train_loader, valid_loader, num_epochs=10, learning_rate=0.001):\n",
        "    criterion = nn.CrossEntropyLoss()  # Assuming you are using CrossEntropyLoss for classification\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        for images, labels in train_loader:\n",
        "            # Convert labels to a tensor\n",
        "            labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for images, labels in valid_loader:\n",
        "                # Convert labels to a tensor\n",
        "                labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "                # Forward pass for validation loss computation\n",
        "                outputs = model(images)\n",
        "                validation_loss = criterion(outputs, labels)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Validation Loss: {validation_loss.item()}\")\n"
      ],
      "metadata": {
        "id": "lW2AFMMsV1x9"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage for a regression task\n",
        "num_output_nodes = 1  # Since it's a regression task\n",
        "resnet50_model = resnet50(num_output_nodes)\n",
        "train_model(resnet50_model, train_loader, valid_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "HMn4L7qOcD0u",
        "outputId": "ed572b30-b1f1-4858-cc72-91ab2b1d6490"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-59-0760623f4067>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels, dtype=torch.long)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-9d52ee2af738>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnum_output_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# Since it's a regression task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresnet50_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet50\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_output_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet50_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-59-0760623f4067>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, valid_loader, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m# Backward pass and optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1180\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3053\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3055\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Target 1933 is out of bounds."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Similarly, you can do the same for ResNet-101\n",
        "resnet101_model = resnet101(num_output_nodes)\n",
        "train_model(resnet101_model, train_loader, valid_loader)"
      ],
      "metadata": {
        "id": "SWxR9G3icXFb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}